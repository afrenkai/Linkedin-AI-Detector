# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py

GPT4_REGEX = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}++| ?\p{N}++| ?[^\s\p{L}\p{N}]++|\s++$|\s+(?!\S)|\s"""
DEFAULT_SPECIAL_TOKENS = ["<BOW>", "<EOW>", "<PAD>", "<UNK>", "<CLS>"]

MLP_EXPANSION_FACTOR = 4
DEFAULT_PAD_TOKEN_ID = 0
DEFAULT_BOS_TOKEN_ID = 1
DEFAULT_EOS_TOKEN_ID = 2
DEFAULT_OOS_TOKEN_ID = 3
DEFAULT_HIDDEN_SIZE = 1024
DEFAULT_NUM_HEADS = 8
DEFAULT_NUM_LAYERS = 8
DEFAULT_VOCAB_SIZE = 65536
DEFAULT_BLOCK_SIZE = 1024
DEFAULT_RMS_NORM_EPS = 1e-06
DEFAULT_GRAD_CLIP_NORM = 1.0
DEFAULT_LOG_FREQUENCY = 100
DEFAULT_EPOCHS = 3
DEFAULT_BATCH_SIZE = 4
DEFAULT_LEARNING_RATE = 2e-4
DEFAULT_WARMUP_STEPS = 1000
DEFAULT_GRAD_ACCUM = 8
DEFAULT_WEIGHT_DECAY = 0.1
DEFAULT_BETA_1 = 0.9
DEFAULT_BETA_2 = 0.95
OBJECTIVE_EPOCHS = 5
DEFAULT_EVAL_EVERY = 500
DEFAULT_SAVE_EVERY = 1000
DEFAULT_TRAIN_VAL_SPLIT_RATIO = 0.9
TUNING_TRAIN_RATIO = 0.8
TUNING_EVAL_RATIO = 0.1
TUNING_TEST_RATIO = 0.1
MIN_CHUNK_SIZE = 2
DEFAULT_PAD_TOKEN_ID = 0
DEFAULT_BPE_MERGES = 100
TOKEN_CLEANUP_REGEX = r'(<[\w]{3}>)'

JSON_INDENT = 2
MB_CONVERSION_FACTOR = 1024 * 1024  # on the lowks its been like 3 years since intro to cs but i think its 1024 * 1024...